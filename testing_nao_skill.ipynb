{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAO correlations with energy variables ###\n",
    "\n",
    "Exploring how well the NAO correlates with energy variables on seasonal to decadal timescales during the winter (ONDJFM, DJFM, or DJF). Using the following datasets:\n",
    "\n",
    "* CLEARHEADS - ERA5-derived energy time series, includes offshore wind in EEZs and Heating Degree Days.\n",
    "* ERA5 - reanalysis product for deriving the NAO indices at different timescales.\n",
    "* ENTSO-E - shorter observed time series of capacity factors and other energy variables. For ground truthing the CLEARHEADS data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import local modules\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Import third-party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import iris\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cdsapi\n",
    "import xesmf as xe\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/users/benhutch/energy-met-corr\")\n",
    "import dictionaries_em as dicts\n",
    "\n",
    "sys.path.append(\"/home/users/benhutch/skill-maps/python\")\n",
    "import functions as fnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading ERA5 data ###\n",
    "\n",
    "For calculating the NAO index, we want to query the CDS API for ERA5 data:\n",
    "\n",
    "* From 1950-2023\n",
    "* For ONDJFM\n",
    "* Monthly-means\n",
    "\n",
    "*Note - this data should be regridded before comparison with the CLEARHEADS/ENTSO-E data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a new client\n",
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dictionary for the ERA5 request\n",
    "era5_request_dict = {\n",
    "    'variable': 'mean_sea_level_pressure',\n",
    "    'product_type': 'monthly_averaged_reanalysis',\n",
    "    'year': [x for x in map(str, range(1950, 2023))],\n",
    "    'month': [1, 2, 3, 10, 11, 12],\n",
    "    'format': 'netcdf',\n",
    "    'time': '00:00'\n",
    "}\n",
    "\n",
    "# Print the request dictionary\n",
    "print(era5_request_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the target directory\n",
    "# target_dir = '/gws/nopw/j04/canari/users/benhutch/ERA5'\n",
    "\n",
    "# # Assert that the target directory exists\n",
    "# assert os.path.exists(target_dir)\n",
    "\n",
    "# # Assert that the target directory is not empty\n",
    "# assert len(os.listdir(target_dir)) > 0\n",
    "\n",
    "# # Set up the target file\n",
    "# target_file = os.path.join(target_dir, 'era5_mslp_monthly_1950_2022_ONDJFM.nc')\n",
    "\n",
    "# # Print the target file\n",
    "# print(target_file)\n",
    "\n",
    "# # If the target file does not exist, download the data\n",
    "# if not os.path.exists(target_file):\n",
    "#     c.retrieve(\n",
    "#         'reanalysis-era5-single-levels',\n",
    "#         era5_request_dict,\n",
    "#         target_file)\n",
    "# else:\n",
    "#     print('The target file already exists: {}'.format(target_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to plot the observed spatial correlations between the NAO and 10m wind speeds and precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to calculate the stats\n",
    "def calc_nao_spatial_corr(season: str,\n",
    "                          forecast_range: str,\n",
    "                          start_year: int,\n",
    "                          end_year: int,\n",
    "                          corr_var: str = \"tos\",\n",
    "                          corr_var_obs_file: str = dicts.regrid_file_pr,\n",
    "                          nao_obs_var: str = \"msl\",\n",
    "                          nao_obs_file: str = dicts.regrid_file,\n",
    "                          nao_n_grid: dict = dicts.iceland_grid_corrected,\n",
    "                          nao_s_grid: dict = dicts.azores_grid_corrected,\n",
    "                          sig_threshold: float = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the spatial correlations between the NAO index (winter default) \n",
    "    and the variable to correlate for the observations.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "\n",
    "    season: str\n",
    "        The season to calculate the correlation for.\n",
    "\n",
    "    forecast_range: str\n",
    "        The forecast range to calculate the correlation for.\n",
    "\n",
    "    start_year: int\n",
    "        The start year to calculate the correlation for.\n",
    "\n",
    "    end_year: int\n",
    "        The end year to calculate the correlation for.\n",
    "\n",
    "    corr_var: str\n",
    "        The variable to correlate with the NAO index.\n",
    "\n",
    "    corr_var_obs_file: str\n",
    "        The file containing the observations of the variable to correlate.\n",
    "\n",
    "    nao_obs_var: str\n",
    "        The variable to use for the NAO index.\n",
    "\n",
    "    nao_obs_file: str\n",
    "        The file containing the observations of the NAO index.\n",
    "\n",
    "    nao_n_grid: dict\n",
    "        The dictionary containing the grid information for the northern node\n",
    "        of the winter NAO index.\n",
    "\n",
    "    nao_s_grid: dict\n",
    "        The dictionary containing the grid information for the southern node\n",
    "        of the winter NAO index.\n",
    "\n",
    "    sig_threshold: float\n",
    "        The significance threshold for the correlation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    stats_dict: dict\n",
    "        The dictionary containing the correlation statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the mdi\n",
    "    mdi = -9999.0\n",
    "\n",
    "    # Form the dictionary\n",
    "    stats_dict = {\n",
    "        \"nao\": [],\n",
    "        \"corr_var_ts\": [],\n",
    "        \"corr_var\": corr_var,\n",
    "        \"corr_nao_var\": [],\n",
    "        \"corr_nao_var_pval\": [],\n",
    "        \"init_years\": [],\n",
    "        \"valid_years\": [],\n",
    "        \"lats\": [],\n",
    "        \"lons\": [],\n",
    "        \"season\": season,\n",
    "        \"forecast_range\": forecast_range,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"sig_threshold\": sig_threshold\n",
    "    }\n",
    "\n",
    "    # Set up the init years\n",
    "    stats_dict[\"init_years\"] = np.arange(start_year, end_year + 1)\n",
    "\n",
    "    # Assert that the season is a winter season\n",
    "    assert season in [\"DJF\", \"ONDJFM\", \"DJFM\"], \"The season must be a winter season.\"\n",
    "\n",
    "    # Assert that the forecast range is a valid forecast range\n",
    "    assert \"-\" in forecast_range, \"The forecast range must be a valid forecast range.\"\n",
    "\n",
    "    # Set up the lons and lats for the south grid\n",
    "    s_lon1, s_lon2 = nao_s_grid[\"lon1\"], nao_s_grid[\"lon2\"]\n",
    "    s_lat1, s_lat2 = nao_s_grid[\"lat1\"], nao_s_grid[\"lat2\"]\n",
    "\n",
    "    # and for the north grid\n",
    "    n_lon1, n_lon2 = nao_n_grid[\"lon1\"], nao_n_grid[\"lon2\"]\n",
    "    n_lat1, n_lat2 = nao_n_grid[\"lat1\"], nao_n_grid[\"lat2\"]\n",
    "\n",
    "    # First check that the file exists for psl\n",
    "    assert os.path.exists(corr_var_obs_file), \"The file for the variable to correlate does not exist.\"\n",
    "\n",
    "    # Check that the file exists for the NAO index\n",
    "    assert os.path.exists(nao_obs_file), \"The file for the NAO index does not exist.\"\n",
    "\n",
    "    # Load the observations for psl\n",
    "    psl = fnc.load_obs(variable=nao_obs_var,\n",
    "                   regrid_obs_path=nao_obs_file)\n",
    "    \n",
    "    # Load the observations for the matching var\n",
    "    corr_var_field = fnc.load_obs(variable=corr_var,\n",
    "                        regrid_obs_path=corr_var_obs_file)\n",
    "    \n",
    "    # extract the months\n",
    "    months = dicts.season_month_map[season]\n",
    "\n",
    "    # Set up an iris constraint for the start and end years\n",
    "    start_date = datetime(int(start_year), months[0], 1)\n",
    "    end_date = datetime(int(end_year), months[-1], 31)\n",
    "\n",
    "    # Form the constraint\n",
    "    time_constraint = iris.Constraint(time=lambda cell: start_date <= cell.point <= end_date)\n",
    "\n",
    "    # Apply the constraint\n",
    "    psl = psl.extract(time_constraint)\n",
    "\n",
    "    # Apply the constraint\n",
    "    corr_var_field = corr_var_field.extract(time_constraint)\n",
    "\n",
    "    # Set up the constrain for months\n",
    "    month_constraint = iris.Constraint(time=lambda cell: cell.point.month in months)\n",
    "\n",
    "    # Apply the constraint\n",
    "    psl = psl.extract(month_constraint)\n",
    "    \n",
    "    # Apply the constraint\n",
    "    corr_var_field = corr_var_field.extract(month_constraint)\n",
    "    \n",
    "    # Calculate the climatology by collapsing the time dimension\n",
    "    psl_clim = psl.collapsed(\"time\", iris.analysis.MEAN)\n",
    "\n",
    "    # Calculate the climatology by collapsing the time dimension\n",
    "    corr_var_clim = corr_var_field.collapsed(\"time\", iris.analysis.MEAN)\n",
    "\n",
    "    # Calculate the anomalies\n",
    "    psl_anom = psl - psl_clim\n",
    "\n",
    "    # Calculate the anomalies\n",
    "    corr_var_anom = corr_var_field - corr_var_clim\n",
    "\n",
    "    # Calculate the annual mean anoms\n",
    "    psl_anom = fnc.calculate_annual_mean_anomalies(obs_anomalies=psl_anom,\n",
    "                                               season=season)\n",
    "    \n",
    "    # Calculate the annual mean anoms\n",
    "    corr_var_anom = fnc.calculate_annual_mean_anomalies(obs_anomalies=corr_var_anom,\n",
    "                                               season=season)\n",
    "    \n",
    "    # # Print psl anom at the first time step\n",
    "    # print(\"psl anom at the first time step: \", psl_anom.isel(time=0).values)\n",
    "    \n",
    "    # # print corr_var anom at the first time step\n",
    "    # print(\"corr_var anom at the first time step: \", corr_var_anom.isel(time=0).values)\n",
    "\n",
    "    # Select the forecast range\n",
    "    psl_anom = fnc.select_forecast_range(obs_anomalies_annual=psl_anom,\n",
    "                                        forecast_range=forecast_range)\n",
    "    \n",
    "    # Select the forecast range\n",
    "    corr_var_anom = fnc.select_forecast_range(obs_anomalies_annual=corr_var_anom,\n",
    "                                        forecast_range=forecast_range)\n",
    "    \n",
    "    # Years 2-9, gives an 8 year running mean\n",
    "    # Which means that the first 4 years (1960, 1961, 1962, 1963) are not valid\n",
    "    # And the last 4 years (2011, 2012, 2013, 2014) are not valid\n",
    "    # extract the digits from the forecast range\n",
    "    digits = [int(x) for x in forecast_range.split(\"-\")]\n",
    "    # Find the absolute difference between the digits\n",
    "    diff = abs(digits[0] - digits[1])\n",
    "\n",
    "    # Find the number of invalid years after centred running mean on each end\n",
    "    n_invalid_years = diff + 1 / 2\n",
    "\n",
    "    # Subset corr_var_anom to remove the invalid years\n",
    "    corr_var_anom = corr_var_anom.isel(time=slice(int(n_invalid_years), -int(n_invalid_years)))\n",
    "    \n",
    "    # # Loop over the years in psl_anom\n",
    "    # for year in psl_anom.time.dt.year.values:\n",
    "    #     # Extract the data for the year\n",
    "    #     psl_anom_year = psl_anom.sel(time=f\"{year}\")\n",
    "\n",
    "    #     # If there are any NaNs, log it\n",
    "    #     if np.isnan(psl_anom_year).any():\n",
    "    #         print(\"There are NaNs in the psl_anom_year for year: \", year)\n",
    "    #         # if all values are NaN, then continue\n",
    "    #         if np.all(np.isnan(psl_anom_year)):\n",
    "    #             print(\"All values are NaN for year: \", year)\n",
    "    #             print(\"Removing the year: \", year)\n",
    "    #             # Remove the year from the psl_anom\n",
    "    #             psl_anom = psl_anom.sel(time=psl_anom.time.dt.year != year)\n",
    "\n",
    "    # # Loop over the first 10 years and last 10 years in psl_anom\n",
    "    # for year in corr_var_anom.time.dt.year.values[:10]:\n",
    "    #     # Extract the data for the year\n",
    "    #     corr_var_anom_year = corr_var_anom.sel(time=f\"{year}\")\n",
    "\n",
    "    #     # If there are any NaNs, log it\n",
    "    #     if np.isnan(corr_var_anom_year).any():\n",
    "    #         print(\"There are NaNs in the corr_var_anom_year for year: \", year)\n",
    "    #         # if all values are NaN, then continue\n",
    "    #         if np.all(np.isnan(corr_var_anom_year)):\n",
    "    #             print(\"All values are NaN for year: \", year)\n",
    "    #             print(\"Removing the year: \", year)\n",
    "    #             # Remove the year from the psl_anom\n",
    "    #             corr_var_anom = corr_var_anom.sel(time=corr_var_anom.time.dt.year != year)\n",
    "\n",
    "    # # Loop over the last 10 years in psl_anom\n",
    "    # for year in corr_var_anom.time.dt.year.values[-10:]:\n",
    "    #     # Extract the data for the year\n",
    "    #     corr_var_anom_year = corr_var_anom.sel(time=f\"{year}\")\n",
    "\n",
    "    #     # If there are any NaNs, log it\n",
    "    #     if np.isnan(corr_var_anom_year).any():\n",
    "    #         print(\"There are NaNs in the corr_var_anom_year for year: \", year)\n",
    "    #         # if all values are NaN, then continue\n",
    "    #         if np.all(np.isnan(corr_var_anom_year)):\n",
    "    #             print(\"All values are NaN for year: \", year)\n",
    "    #             print(\"Removing the year: \", year)\n",
    "    #             # Remove the year from the psl_anom\n",
    "    #             corr_var_anom = corr_var_anom.sel(time=corr_var_anom.time.dt.year != year)\n",
    "    \n",
    "    # print the type of psl_anom\n",
    "    print(\"type of psl_anom: \", type(psl_anom))\n",
    "\n",
    "    # print the type of corr_var_anom\n",
    "    print(\"type of corr_var_anom: \", type(corr_var_anom))\n",
    "\n",
    "    # Extract the years for psl anom\n",
    "    # years_psl = psl_anom.time.dt.year.values\n",
    "    years_corr_var = corr_var_anom.time.dt.year.values\n",
    "\n",
    "    # # Set the time axis for psl_anom to the years\n",
    "    # psl_anom = psl_anom.assign_coords(time=years_psl)\n",
    "\n",
    "    # Set the time axis for corr_var_anom to the years\n",
    "    corr_var_anom = corr_var_anom.assign_coords(time=years_corr_var)\n",
    "\n",
    "    # Lat goes from 90 to -90\n",
    "    # Lon goes from 0 to 360\n",
    "\n",
    "    # # If s_lat1 is smaller than s_lat2, then we need to switch them\n",
    "    # if s_lat1 < s_lat2:\n",
    "    #     s_lat1, s_lat2 = s_lat2, s_lat1\n",
    "\n",
    "    # # If n_lat1 is smaller than n_lat2, then we need to switch them\n",
    "    # if n_lat1 < n_lat2:\n",
    "    #     n_lat1, n_lat2 = n_lat2, n_lat1\n",
    "\n",
    "    # # Asert that the lons are within the range of 0 to 360\n",
    "    # assert 0 <= s_lon1 <= 360, \"The southern lon is not within the range of 0 to 360.\"\n",
    "\n",
    "    # # Asert that the lons are within the range of 0 to 360\n",
    "    # assert 0 <= s_lon2 <= 360, \"The southern lon is not within the range of 0 to 360.\"\n",
    "\n",
    "    # # Asert that the lons are within the range of 0 to 360\n",
    "    # assert 0 <= n_lon1 <= 360, \"The northern lon is not within the range of 0 to 360.\"\n",
    "\n",
    "    # # Asert that the lons are within the range of 0 to 360\n",
    "    # assert 0 <= n_lon2 <= 360, \"The northern lon is not within the range of 0 to 360.\"\n",
    "\n",
    "    # Constraint the psl_anom to the south grid\n",
    "    psl_anom_s = psl_anom.sel(lon=slice(s_lon1, s_lon2),\n",
    "                               lat=slice(s_lat1, s_lat2)\n",
    "                               ).mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "    # Constraint the psl_anom to the north grid\n",
    "    psl_anom_n = psl_anom.sel(lon=slice(n_lon1, n_lon2),\n",
    "                               lat=slice(n_lat1, n_lat2)\n",
    "                               ).mean(dim=[\"lat\", \"lon\"])\n",
    "    \n",
    "    # Calculate the nao index azores - iceland\n",
    "    nao_index = psl_anom_s - psl_anom_n\n",
    "\n",
    "    # Loop over the first 10 years and last 10 years in nao_index\n",
    "    # for year in nao_index.time.dt.year.values:\n",
    "    #     # Extract the data for the year\n",
    "    #     nao_index_year = nao_index.sel(time=f\"{year}\")\n",
    "\n",
    "    #     # If there are any NaNs, log it\n",
    "    #     if np.isnan(nao_index_year).any():\n",
    "    #         print(\"There are NaNs in the nao_index_year for year: \", year)\n",
    "    #         # if all values are NaN, then continue\n",
    "    #         if np.all(np.isnan(nao_index_year)):\n",
    "    #             print(\"All values are NaN for year: \", year)\n",
    "    #             print(\"Removing the year: \", year)\n",
    "    #             # Remove the year from the nao_index\n",
    "    #             nao_index = nao_index.sel(time=nao_index.time.dt.year != year)\n",
    "\n",
    "    # Subset the nao_index to remove the invalid years\n",
    "    nao_index = nao_index.isel(time=slice(int(n_invalid_years), -int(n_invalid_years)))\n",
    "\n",
    "    # Extract the years for nao_index\n",
    "    years_nao = nao_index.time.dt.year.values\n",
    "\n",
    "    # Extract the years for corr_var_anom\n",
    "    years_corr_var = corr_var_anom.time.values\n",
    "\n",
    "    # Assert that the years are the same\n",
    "    assert np.array_equal(years_nao, years_corr_var), \"The years for the NAO index and the variable to correlate are not the same.\"\n",
    "\n",
    "    # Set the valid years\n",
    "    stats_dict[\"valid_years\"] = years_nao\n",
    "\n",
    "    # extract tyhe lats and lons\n",
    "    lats = corr_var_anom.lat.values\n",
    "\n",
    "    # extract the lons\n",
    "    lons = corr_var_anom.lon.values\n",
    "\n",
    "    # Store the lats and lons in the dictionary\n",
    "    stats_dict[\"lats\"] = lats\n",
    "    stats_dict[\"lons\"] = lons\n",
    "\n",
    "    # Extract the values for the NAO index\n",
    "    nao_index_values = nao_index.values\n",
    "\n",
    "    # Extract the values for the variable to correlate\n",
    "    corr_var_anom_values = corr_var_anom.values\n",
    "\n",
    "    # Store the nao index values in the dictionary\n",
    "    stats_dict[\"nao\"] = nao_index_values\n",
    "\n",
    "    # Store the variable to correlate values in the dictionary\n",
    "    stats_dict[\"corr_var_ts\"] = corr_var_anom_values\n",
    "\n",
    "    # # Create an empty array with the correct shape for the correlation\n",
    "    # corr_nao_var = np.empty((len(lats), len(lons)))\n",
    "\n",
    "    # # Create an empty array with the correct shape for the p-value\n",
    "    # corr_nao_var_pval = np.empty((len(lats), len(lons)))\n",
    "\n",
    "    # # Loop over the lats\n",
    "    # for i in tqdm(range(len(lats)), desc=\"Calculating spatial correlation\"):\n",
    "    #     # Loop over the lons\n",
    "    #     for j in range(len(lons)):\n",
    "    #         # Extract the values for the variable to correlate\n",
    "    #         corr_var_anom_values = corr_var_anom.values[:, i, j]\n",
    "\n",
    "    #         # Calculate the correlation\n",
    "    #         corr, pval = pearsonr(nao_index_values, corr_var_anom_values)\n",
    "\n",
    "    #         # Store the correlation in the array\n",
    "    #         corr_nao_var[i, j] = corr\n",
    "\n",
    "    #         # Store the p-value in the array\n",
    "    #         corr_nao_var_pval[i, j] = pval\n",
    "\n",
    "    # # Store the correlation in the dictionary\n",
    "    # stats_dict[\"corr_nao_var\"] = corr_nao_var\n",
    "\n",
    "    # # Store the p-value in the dictionary\n",
    "    # stats_dict[\"corr_nao_var_pval\"] = corr_nao_var_pval\n",
    "\n",
    "    # return none\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this function\n",
    "stats_dict = calc_nao_spatial_corr(\n",
    "    season=\"ONDJFM\",\n",
    "    forecast_range=\"2-9\",\n",
    "    start_year=1960,\n",
    "    end_year=2014,\n",
    "    corr_var=\"pr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the nao index\n",
    "nao = stats_dict[\"nao\"]\n",
    "\n",
    "# Extract the corr var anomalies\n",
    "corr_var_ts = stats_dict[\"corr_var_ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the nao index\n",
    "plt.plot(nao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Create an empty array with the correct shape\n",
    "corr_array = np.empty([corr_var_ts.shape[1], # lat\n",
    "                          corr_var_ts.shape[2]]) # lon\n",
    "\n",
    "# Same for the p-values\n",
    "pval_array = np.empty([corr_var_ts.shape[1], # lat\n",
    "                          corr_var_ts.shape[2]]) # lon\n",
    "\n",
    "# Loop over the lats\n",
    "for lat in tqdm(range(corr_var_ts.shape[1])):\n",
    "    # Loop over the lons\n",
    "    for lon in range(corr_var_ts.shape[2]):\n",
    "         # Extract the corr_var_ts for the lat and lon\n",
    "         corr_var_anom_values_lat_lon = corr_var_ts[:, lat, lon]\n",
    "\n",
    "         # Replace NaNs with 0\n",
    "         corr_var_anom_values_lat_lon = np.nan_to_num(corr_var_anom_values_lat_lon, nan=0)\n",
    "\n",
    "         # Calculate the correlation\n",
    "         corr, pval = pearsonr(nao, corr_var_anom_values_lat_lon)\n",
    "\n",
    "         # Assign the correlation to the array\n",
    "         corr_array[lat, lon] = corr\n",
    "\n",
    "         # Assign the p-value to the array\n",
    "         pval_array[lat, lon] = pval\n",
    "\n",
    "# Print the shape of the corr_array\n",
    "print(\"shape of corr_array: \", corr_array.shape)\n",
    "print(\"shape of pval_array: \", pval_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple function for plotting the correlation\n",
    "def plot_corr(corr_array: np.ndarray,\n",
    "                pval_array: np.ndarray,\n",
    "                lats: np.ndarray,\n",
    "                lons: np.ndarray,\n",
    "                sig_threshold: float = 0.05):\n",
    "        \"\"\"\n",
    "        Plots the correlation and p-values for the spatial correlation.\n",
    "    \n",
    "        Args:\n",
    "        -----\n",
    "    \n",
    "        corr_array: np.ndarray\n",
    "            The array containing the correlation values.\n",
    "    \n",
    "        pval_array: np.ndarray\n",
    "            The array containing the p-values.\n",
    "    \n",
    "        lats: np.ndarray\n",
    "            The array containing the latitudes.\n",
    "    \n",
    "        lons: np.ndarray\n",
    "            The array containing the longitudes.\n",
    "    \n",
    "        sig_threshold: float\n",
    "            The significance threshold for the correlation.\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "    \n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Plot these values\n",
    "        # Set up a single subplot\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Set up the projection\n",
    "        proj = ccrs.PlateCarree(central_longitude=0)\n",
    "\n",
    "        # Focus on the euro-atlantic region\n",
    "        lat1_grid, lat2_grid = 20, 80\n",
    "        lon1_grid, lon2_grid = -100, 40\n",
    "\n",
    "\n",
    "        lat1_idx_grid = np.argmin(np.abs(lats - lat1_grid))\n",
    "        lat2_idx_grid = np.argmin(np.abs(lats - lat2_grid))\n",
    "\n",
    "        lon1_idx_grid = np.argmin(np.abs(lons - lon1_grid))\n",
    "        lon2_idx_grid = np.argmin(np.abs(lons - lon2_grid))\n",
    "\n",
    "        # Print the indices\n",
    "        print(\"lon1_idx_grid: \", lon1_idx_grid)\n",
    "        print(\"lon2_idx_grid: \", lon2_idx_grid)\n",
    "        print(\"lat1_idx_grid: \", lat1_idx_grid)\n",
    "        print(\"lat2_idx_grid: \", lat2_idx_grid)\n",
    "\n",
    "        # # If lat1_idx_grid is greater than lat2_idx_grid, then switch them\n",
    "        # if lat1_idx_grid > lat2_idx_grid:\n",
    "        #     lat1_idx_grid, lat2_idx_grid = lat2_idx_grid, lat1_idx_grid\n",
    "\n",
    "        # Print the indices\n",
    "        print(\"lon1_idx_grid: \", lon1_idx_grid)\n",
    "        print(\"lon2_idx_grid: \", lon2_idx_grid)\n",
    "        print(\"lat1_idx_grid: \", lat1_idx_grid)\n",
    "        print(\"lat2_idx_grid: \", lat2_idx_grid)\n",
    "\n",
    "        # Constrain the lats and lons to the grid\n",
    "        lats = lats[lat1_idx_grid:lat2_idx_grid]\n",
    "        lons = lons[lon1_idx_grid:lon2_idx_grid]\n",
    "\n",
    "        # Constrain the corr_array to the grid\n",
    "        corr_array = corr_array[lat1_idx_grid:lat2_idx_grid, lon1_idx_grid:lon2_idx_grid]\n",
    "\n",
    "        # Constrain the pval_array to the grid\n",
    "        pval_array = pval_array[lat1_idx_grid:lat2_idx_grid, lon1_idx_grid:lon2_idx_grid]\n",
    "\n",
    "        # Set up the contour levels\n",
    "        clevs = np.arange(-1.0, 1.1, 0.1)\n",
    "\n",
    "        # Set up the axis\n",
    "        ax = plt.axes(projection=proj)\n",
    "\n",
    "        # Include coastlines\n",
    "        ax.coastlines()\n",
    "\n",
    "        # # Shift lon back to -180 to 180\n",
    "        # lons = lons - 180\n",
    "\n",
    "        # Set up the contour plot\n",
    "        cf = ax.contourf(lons, lats, corr_array, clevs, transform=proj, cmap=\"RdBu_r\")\n",
    "\n",
    "        # if any of the p values are greater or less than the significance threshold\n",
    "        sig_threshold = 0.05\n",
    "        pval_array[(pval_array > sig_threshold) & (pval_array < 1 - sig_threshold)] = np.nan\n",
    "\n",
    "        # Plot the p-values\n",
    "        ax.contourf(lons, lats, pval_array, hatches=[\"....\"], alpha=0.0, transform=proj)\n",
    "\n",
    "        # Set up the colorbar\n",
    "        cbar = plt.colorbar(cf, ax=ax, orientation=\"horizontal\", pad=0.05, shrink=0.8)\n",
    "\n",
    "        # Set up the colorbar label\n",
    "        cbar.set_label(\"correlation coefficient\")\n",
    "\n",
    "        # Add a title\n",
    "        ax.set_title(\"Corr(obs NAO, obs precip)\")\n",
    "\n",
    "        # Render the plot\n",
    "        plt.show()\n",
    "\n",
    "        # Return none\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "plot_corr(corr_array=corr_array,\n",
    "            pval_array=pval_array,\n",
    "            lats=stats_dict[\"lats\"],\n",
    "            lons=stats_dict[\"lons\"],\n",
    "            sig_threshold=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
