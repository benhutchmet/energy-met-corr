{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAO correlations with energy variables ###\n",
    "\n",
    "Exploring how well the NAO correlates with energy variables on seasonal to decadal timescales during the winter (ONDJFM, DJFM, or DJF). Using the following datasets:\n",
    "\n",
    "* CLEARHEADS - ERA5-derived energy time series, includes offshore wind in EEZs and Heating Degree Days.\n",
    "* ERA5 - reanalysis product for deriving the NAO indices at different timescales.\n",
    "* ENTSO-E - shorter observed time series of capacity factors and other energy variables. For ground truthing the CLEARHEADS data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import local modules\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Import third-party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import iris\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cdsapi\n",
    "import xesmf as xe\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/users/benhutch/energy-met-corr\")\n",
    "import dictionaries_em as dicts\n",
    "\n",
    "sys.path.append(\"/home/users/benhutch/skill-maps/python\")\n",
    "import functions as fnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading ERA5 data ###\n",
    "\n",
    "For calculating the NAO index, we want to query the CDS API for ERA5 data:\n",
    "\n",
    "* From 1950-2023\n",
    "* For ONDJFM\n",
    "* Monthly-means\n",
    "\n",
    "*Note - this data should be regridded before comparison with the CLEARHEADS/ENTSO-E data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a new client\n",
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dictionary for the ERA5 request\n",
    "era5_request_dict = {\n",
    "    'variable': 'mean_sea_level_pressure',\n",
    "    'product_type': 'monthly_averaged_reanalysis',\n",
    "    'year': [x for x in map(str, range(1950, 2023))],\n",
    "    'month': [1, 2, 3, 10, 11, 12],\n",
    "    'format': 'netcdf',\n",
    "    'time': '00:00'\n",
    "}\n",
    "\n",
    "# Print the request dictionary\n",
    "print(era5_request_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the target directory\n",
    "# target_dir = '/gws/nopw/j04/canari/users/benhutch/ERA5'\n",
    "\n",
    "# # Assert that the target directory exists\n",
    "# assert os.path.exists(target_dir)\n",
    "\n",
    "# # Assert that the target directory is not empty\n",
    "# assert len(os.listdir(target_dir)) > 0\n",
    "\n",
    "# # Set up the target file\n",
    "# target_file = os.path.join(target_dir, 'era5_mslp_monthly_1950_2022_ONDJFM.nc')\n",
    "\n",
    "# # Print the target file\n",
    "# print(target_file)\n",
    "\n",
    "# # If the target file does not exist, download the data\n",
    "# if not os.path.exists(target_file):\n",
    "#     c.retrieve(\n",
    "#         'reanalysis-era5-single-levels',\n",
    "#         era5_request_dict,\n",
    "#         target_file)\n",
    "# else:\n",
    "#     print('The target file already exists: {}'.format(target_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to plot the observed spatial correlations between the NAO and 10m wind speeds and precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to calculate the stats\n",
    "def calc_nao_spatial_corr(season: str,\n",
    "                          forecast_range: str,\n",
    "                          start_year: int,\n",
    "                          end_year: int,\n",
    "                          corr_var: str = \"si10\",\n",
    "                          corr_var_obs_file: str = \"/home/users/benhutch/ERA5/adaptor.mars.internal-1691509121.3261805-29348-4-3a487c76-fc7b-421f-b5be-7436e2eb78d7.nc\",\n",
    "                          nao_obs_var: str = \"msl\",\n",
    "                          nao_obs_file: str = \"/home/users/benhutch/ERA5/adaptor.mars.internal-1691509121.3261805-29348-4-3a487c76-fc7b-421f-b5be-7436e2eb78d7.nc\",\n",
    "                          nao_n_grid: dict = dicts.iceland_grid,\n",
    "                          nao_s_grid: dict = dicts.azores_grid,\n",
    "                          sig_threshold: float = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the spatial correlations between the NAO index (winter default) \n",
    "    and the variable to correlate for the observations.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "\n",
    "    season: str\n",
    "        The season to calculate the correlation for.\n",
    "\n",
    "    forecast_range: str\n",
    "        The forecast range to calculate the correlation for.\n",
    "\n",
    "    start_year: int\n",
    "        The start year to calculate the correlation for.\n",
    "\n",
    "    end_year: int\n",
    "        The end year to calculate the correlation for.\n",
    "\n",
    "    corr_var: str\n",
    "        The variable to correlate with the NAO index.\n",
    "\n",
    "    corr_var_obs_file: str\n",
    "        The file containing the observations of the variable to correlate.\n",
    "\n",
    "    nao_obs_var: str\n",
    "        The variable to use for the NAO index.\n",
    "\n",
    "    nao_obs_file: str\n",
    "        The file containing the observations of the NAO index.\n",
    "\n",
    "    nao_n_grid: dict\n",
    "        The dictionary containing the grid information for the northern node\n",
    "        of the winter NAO index.\n",
    "\n",
    "    nao_s_grid: dict\n",
    "        The dictionary containing the grid information for the southern node\n",
    "        of the winter NAO index.\n",
    "\n",
    "    sig_threshold: float\n",
    "        The significance threshold for the correlation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    stats_dict: dict\n",
    "        The dictionary containing the correlation statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the mdi\n",
    "    mdi = -9999.0\n",
    "\n",
    "    # Form the dictionary\n",
    "    stats_dict = {\n",
    "        \"nao\": [],\n",
    "        \"corr_var_ts\": [],\n",
    "        \"corr_var\": corr_var,\n",
    "        \"corr_nao_var\": [],\n",
    "        \"corr_nao_var_pval\": [],\n",
    "        \"init_years\": [],\n",
    "        \"valid_years\": [],\n",
    "        \"lats\": [],\n",
    "        \"lons\": [],\n",
    "        \"season\": season,\n",
    "        \"forecast_range\": forecast_range,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"sig_threshold\": sig_threshold\n",
    "    }\n",
    "\n",
    "    # Set up the init years\n",
    "    stats_dict[\"init_years\"] = np.arange(start_year, end_year + 1)\n",
    "\n",
    "    # Assert that the season is a winter season\n",
    "    assert season in [\"DJF\", \"ONDJFM\", \"DJFM\"], \"The season must be a winter season.\"\n",
    "\n",
    "    # Assert that the forecast range is a valid forecast range\n",
    "    assert \"-\" in forecast_range, \"The forecast range must be a valid forecast range.\"\n",
    "\n",
    "    # Set up the lons and lats for the south grid\n",
    "    s_lon1, s_lon2 = nao_s_grid[\"lon1\"], nao_s_grid[\"lon2\"]\n",
    "    s_lat1, s_lat2 = nao_s_grid[\"lat1\"], nao_s_grid[\"lat2\"]\n",
    "\n",
    "    # and for the north grid\n",
    "    n_lon1, n_lon2 = nao_n_grid[\"lon1\"], nao_n_grid[\"lon2\"]\n",
    "    n_lat1, n_lat2 = nao_n_grid[\"lat1\"], nao_n_grid[\"lat2\"]\n",
    "\n",
    "    # First check that the file exists for psl\n",
    "    assert os.path.exists(corr_var_obs_file), \"The file for the variable to correlate does not exist.\"\n",
    "\n",
    "    # Check that the file exists for the NAO index\n",
    "    assert os.path.exists(nao_obs_file), \"The file for the NAO index does not exist.\"\n",
    "\n",
    "    # Load the observations for psl\n",
    "    psl = fnc.load_obs(variable=nao_obs_var,\n",
    "                   regrid_obs_path=nao_obs_file)\n",
    "    \n",
    "    # Load the observations for the matching var\n",
    "    corr_var_field = fnc.load_obs(variable=corr_var,\n",
    "                        regrid_obs_path=corr_var_obs_file)\n",
    "    \n",
    "    # extract the months\n",
    "    months = dicts.season_month_map[season]\n",
    "\n",
    "    # Set up an iris constraint for the start and end years\n",
    "    start_date = datetime(int(start_year), months[0], 1)\n",
    "    end_date = datetime(int(end_year), months[-1], 31)\n",
    "\n",
    "    # Form the constraint\n",
    "    time_constraint = iris.Constraint(time=lambda cell: start_date <= cell.point <= end_date)\n",
    "\n",
    "    # Apply the constraint\n",
    "    psl = psl.extract(time_constraint)\n",
    "\n",
    "    # Apply the constraint\n",
    "    corr_var_field = corr_var_field.extract(time_constraint)\n",
    "\n",
    "    # Set up the constrain for months\n",
    "    month_constraint = iris.Constraint(time=lambda cell: cell.point.month in months)\n",
    "\n",
    "    # Apply the constraint\n",
    "    psl = psl.extract(month_constraint)\n",
    "    \n",
    "    # Apply the constraint\n",
    "    corr_var_field = corr_var_field.extract(month_constraint)\n",
    "    \n",
    "    # Calculate the climatology by collapsing the time dimension\n",
    "    psl_clim = psl.collapsed(\"time\", iris.analysis.MEAN)\n",
    "\n",
    "    # Calculate the climatology by collapsing the time dimension\n",
    "    corr_var_clim = corr_var_field.collapsed(\"time\", iris.analysis.MEAN)\n",
    "\n",
    "    # Calculate the anomalies\n",
    "    psl_anom = psl - psl_clim\n",
    "\n",
    "    # Calculate the anomalies\n",
    "    corr_var_anom = corr_var_field - corr_var_clim\n",
    "\n",
    "    # Calculate the annual mean anoms\n",
    "    psl_anom = fnc.calculate_annual_mean_anomalies(obs_anomalies=psl_anom,\n",
    "                                               season=season)\n",
    "    \n",
    "    # Calculate the annual mean anoms\n",
    "    corr_var_anom = fnc.calculate_annual_mean_anomalies(obs_anomalies=corr_var_anom,\n",
    "                                               season=season)\n",
    "    \n",
    "    # # Print psl anom at the first time step\n",
    "    # print(\"psl anom at the first time step: \", psl_anom.isel(time=0).values)\n",
    "    \n",
    "    # # print corr_var anom at the first time step\n",
    "    # print(\"corr_var anom at the first time step: \", corr_var_anom.isel(time=0).values)\n",
    "\n",
    "    # Select the forecast range\n",
    "    psl_anom = fnc.select_forecast_range(obs_anomalies_annual=psl_anom,\n",
    "                                        forecast_range=forecast_range)\n",
    "    \n",
    "    # Select the forecast range\n",
    "    corr_var_anom = fnc.select_forecast_range(obs_anomalies_annual=corr_var_anom,\n",
    "                                        forecast_range=forecast_range)\n",
    "    \n",
    "    # # Loop over the years in psl_anom\n",
    "    # for year in psl_anom.time.dt.year.values:\n",
    "    #     # Extract the data for the year\n",
    "    #     psl_anom_year = psl_anom.sel(time=f\"{year}\")\n",
    "\n",
    "    #     # If there are any NaNs, log it\n",
    "    #     if np.isnan(psl_anom_year).any():\n",
    "    #         print(\"There are NaNs in the psl_anom_year for year: \", year)\n",
    "    #         # if all values are NaN, then continue\n",
    "    #         if np.all(np.isnan(psl_anom_year)):\n",
    "    #             print(\"All values are NaN for year: \", year)\n",
    "    #             print(\"Removing the year: \", year)\n",
    "    #             # Remove the year from the psl_anom\n",
    "    #             psl_anom = psl_anom.sel(time=psl_anom.time.dt.year != year)\n",
    "\n",
    "    # Loop over the first 10 years and last 10 years in psl_anom\n",
    "    for year in corr_var_anom.time.dt.year.values[:10]:\n",
    "        # Extract the data for the year\n",
    "        corr_var_anom_year = corr_var_anom.sel(time=f\"{year}\")\n",
    "\n",
    "        # If there are any NaNs, log it\n",
    "        if np.isnan(corr_var_anom_year).any():\n",
    "            print(\"There are NaNs in the corr_var_anom_year for year: \", year)\n",
    "            # if all values are NaN, then continue\n",
    "            if np.all(np.isnan(corr_var_anom_year)):\n",
    "                print(\"All values are NaN for year: \", year)\n",
    "                print(\"Removing the year: \", year)\n",
    "                # Remove the year from the psl_anom\n",
    "                corr_var_anom = corr_var_anom.sel(time=corr_var_anom.time.dt.year != year)\n",
    "\n",
    "    # Loop over the last 10 years in psl_anom\n",
    "    for year in corr_var_anom.time.dt.year.values[-10:]:\n",
    "        # Extract the data for the year\n",
    "        corr_var_anom_year = corr_var_anom.sel(time=f\"{year}\")\n",
    "\n",
    "        # If there are any NaNs, log it\n",
    "        if np.isnan(corr_var_anom_year).any():\n",
    "            print(\"There are NaNs in the corr_var_anom_year for year: \", year)\n",
    "            # if all values are NaN, then continue\n",
    "            if np.all(np.isnan(corr_var_anom_year)):\n",
    "                print(\"All values are NaN for year: \", year)\n",
    "                print(\"Removing the year: \", year)\n",
    "                # Remove the year from the psl_anom\n",
    "                corr_var_anom = corr_var_anom.sel(time=corr_var_anom.time.dt.year != year)\n",
    "    \n",
    "    # print the type of psl_anom\n",
    "    print(\"type of psl_anom: \", type(psl_anom))\n",
    "\n",
    "    # print the type of corr_var_anom\n",
    "    print(\"type of corr_var_anom: \", type(corr_var_anom))\n",
    "\n",
    "    # Extract the years for psl anom\n",
    "    # years_psl = psl_anom.time.dt.year.values\n",
    "    years_corr_var = corr_var_anom.time.dt.year.values\n",
    "\n",
    "    # # Set the time axis for psl_anom to the years\n",
    "    # psl_anom = psl_anom.assign_coords(time=years_psl)\n",
    "\n",
    "    # Set the time axis for corr_var_anom to the years\n",
    "    corr_var_anom = corr_var_anom.assign_coords(time=years_corr_var)\n",
    "\n",
    "    # Lat goes from 90 to -90\n",
    "    # Lon goes from 0 to 360\n",
    "\n",
    "    # If s_lat1 is smaller than s_lat2, then we need to switch them\n",
    "    if s_lat1 < s_lat2:\n",
    "        s_lat1, s_lat2 = s_lat2, s_lat1\n",
    "\n",
    "    # If n_lat1 is smaller than n_lat2, then we need to switch them\n",
    "    if n_lat1 < n_lat2:\n",
    "        n_lat1, n_lat2 = n_lat2, n_lat1\n",
    "\n",
    "    # Asert that the lons are within the range of 0 to 360\n",
    "    assert 0 <= s_lon1 <= 360, \"The southern longitude is not within the range of 0 to 360.\"\n",
    "\n",
    "    # Asert that the lons are within the range of 0 to 360\n",
    "    assert 0 <= s_lon2 <= 360, \"The southern longitude is not within the range of 0 to 360.\"\n",
    "\n",
    "    # Asert that the lons are within the range of 0 to 360\n",
    "    assert 0 <= n_lon1 <= 360, \"The northern longitude is not within the range of 0 to 360.\"\n",
    "\n",
    "    # Asert that the lons are within the range of 0 to 360\n",
    "    assert 0 <= n_lon2 <= 360, \"The northern longitude is not within the range of 0 to 360.\"\n",
    "\n",
    "    # Constraint the psl_anom to the south grid\n",
    "    psl_anom_s = psl_anom.sel(longitude=slice(s_lon1, s_lon2),\n",
    "                               latitude=slice(s_lat1, s_lat2)\n",
    "                               ).mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # Constraint the psl_anom to the north grid\n",
    "    psl_anom_n = psl_anom.sel(longitude=slice(n_lon1, n_lon2),\n",
    "                               latitude=slice(n_lat1, n_lat2)\n",
    "                               ).mean(dim=[\"latitude\", \"longitude\"])\n",
    "    \n",
    "    # Calculate the nao index azores - iceland\n",
    "    nao_index = psl_anom_s - psl_anom_n\n",
    "\n",
    "    # Loop over the first 10 years and last 10 years in nao_index\n",
    "    for year in nao_index.time.dt.year.values:\n",
    "        # Extract the data for the year\n",
    "        nao_index_year = nao_index.sel(time=f\"{year}\")\n",
    "\n",
    "        # If there are any NaNs, log it\n",
    "        if np.isnan(nao_index_year).any():\n",
    "            print(\"There are NaNs in the nao_index_year for year: \", year)\n",
    "            # if all values are NaN, then continue\n",
    "            if np.all(np.isnan(nao_index_year)):\n",
    "                print(\"All values are NaN for year: \", year)\n",
    "                print(\"Removing the year: \", year)\n",
    "                # Remove the year from the nao_index\n",
    "                nao_index = nao_index.sel(time=nao_index.time.dt.year != year)\n",
    "\n",
    "    # Extract the years for nao_index\n",
    "    years_nao = nao_index.time.dt.year.values\n",
    "\n",
    "    # Extract the years for corr_var_anom\n",
    "    years_corr_var = corr_var_anom.time.values\n",
    "\n",
    "    # Assert that the years are the same\n",
    "    assert np.array_equal(years_nao, years_corr_var), \"The years for the NAO index and the variable to correlate are not the same.\"\n",
    "\n",
    "    # Set the valid years\n",
    "    stats_dict[\"valid_years\"] = years_nao\n",
    "\n",
    "    # extract tyhe lats and lons\n",
    "    lats = corr_var_anom.latitude.values\n",
    "\n",
    "    # extract the lons\n",
    "    lons = corr_var_anom.longitude.values\n",
    "\n",
    "    # Store the lats and lons in the dictionary\n",
    "    stats_dict[\"lats\"] = lats\n",
    "    stats_dict[\"lons\"] = lons\n",
    "\n",
    "    # Extract the values for the NAO index\n",
    "    nao_index_values = nao_index.values\n",
    "\n",
    "    # Extract the values for the variable to correlate\n",
    "    corr_var_anom_values = corr_var_anom.values\n",
    "\n",
    "    # Store the nao index values in the dictionary\n",
    "    stats_dict[\"nao\"] = nao_index_values\n",
    "\n",
    "    # Store the variable to correlate values in the dictionary\n",
    "    stats_dict[\"corr_var_ts\"] = corr_var_anom_values\n",
    "\n",
    "    # Create an empty array with the correct shape for the correlation\n",
    "    corr_nao_var = np.empty((len(lats), len(lons)))\n",
    "\n",
    "    # Create an empty array with the correct shape for the p-value\n",
    "    corr_nao_var_pval = np.empty((len(lats), len(lons)))\n",
    "\n",
    "    # Loop over the lats\n",
    "    for i, lat in tqdm(enumerate(lats)):\n",
    "        # Loop over the lons\n",
    "        for j, lon in enumerate(lons):\n",
    "            # Extract the values for the variable to correlate\n",
    "            corr_var_anom_values = corr_var_anom.values[:, i, j]\n",
    "\n",
    "            # Calculate the correlation\n",
    "            corr, pval = pearsonr(nao_index_values, corr_var_anom_values)\n",
    "\n",
    "            # Store the correlation in the array\n",
    "            corr_nao_var[i, j] = corr\n",
    "\n",
    "            # Store the p-value in the array\n",
    "            corr_nao_var_pval[i, j] = pval\n",
    "\n",
    "    # Store the correlation in the dictionary\n",
    "    stats_dict[\"corr_nao_var\"] = corr_nao_var\n",
    "\n",
    "    # Store the p-value in the dictionary\n",
    "    stats_dict[\"corr_nao_var_pval\"] = corr_nao_var_pval\n",
    "\n",
    "    # return none\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test this function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stats_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_nao_spatial_corr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mONDJFM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2-9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1960\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2014\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorr_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msi10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mcalc_nao_spatial_corr\u001b[0;34m(season, forecast_range, start_year, end_year, corr_var, corr_var_obs_file, nao_obs_var, nao_obs_file, nao_n_grid, nao_s_grid, sig_threshold)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lat \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(lats)):\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# Loop over the lons\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, lon \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lons):\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;66;03m# Extract the values for the variable to correlate\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m         corr_var_anom_values \u001b[38;5;241m=\u001b[39m \u001b[43mcorr_var_anom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m[:, i, j]\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;66;03m# Calculate the correlation\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         corr, pval \u001b[38;5;241m=\u001b[39m pearsonr(nao_index_values, corr_var_anom_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/core/dataarray.py:761\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    754\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/core/variable.py:555\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/core/variable.py:305\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/array/core.py:1700\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1700\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m   1702\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/base.py:377\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/base.py:663\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 663\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/apps/jasmin/jaspy/miniconda_envs/jaspy3.10/m3-4.9.2/envs/jaspy3.10-m3-4.9.2-r20220721/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/apps/jasmin/jaspy/miniconda_envs/jaspy3.10/m3-4.9.2/envs/jaspy3.10-m3-4.9.2-r20220721/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test this function\n",
    "stats_dict = calc_nao_spatial_corr(\n",
    "    season=\"ONDJFM\",\n",
    "    forecast_range=\"2-9\",\n",
    "    start_year=1960,\n",
    "    end_year=2014,\n",
    "    corr_var=\"si10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nao_index_values, corr_var_anom_values = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nao_index_values: \", nao_index_values.shape)\n",
    "print(\"corr_var_anom_values: \", corr_var_anom_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Create an empty array with the correct shape\n",
    "corr_array = np.empty([corr_var_anom_values.shape[1], # lat\n",
    "                          corr_var_anom_values.shape[2]]) # lon\n",
    "\n",
    "# Same for the p-values\n",
    "pval_array = np.empty([corr_var_anom_values.shape[1], # lat\n",
    "                          corr_var_anom_values.shape[2]]) # lon\n",
    "\n",
    "\n",
    "# Loop over the lats\n",
    "for lat in tqdm(range(corr_var_anom_values.shape[1])):\n",
    "    # Loop over the lons\n",
    "    for lon in range(corr_var_anom_values.shape[2]):\n",
    "         # Extract the corr_var_anom_values for the lat and lon\n",
    "         corr_var_anom_values_lat_lon = corr_var_anom_values[:, lat, lon]\n",
    "\n",
    "         # Calculate the correlation\n",
    "         corr, pval = pearsonr(nao_index_values, corr_var_anom_values_lat_lon)\n",
    "\n",
    "         # Assign the correlation to the array\n",
    "         corr_array[lat, lon] = corr\n",
    "\n",
    "         # Assign the p-value to the array\n",
    "         pval_array[lat, lon] = pval\n",
    "\n",
    "# Print the shape of the corr_array\n",
    "print(\"shape of corr_array: \", corr_array.shape)\n",
    "print(\"shape of pval_array: \", pval_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot these values\n",
    "# Set up a single subplot\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the correlation\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the correlation\n",
    "img = ax.imshow(corr_array, \n",
    "                transform=ccrs.PlateCarree(),\n",
    "                cmap=\"RdBu_r\",\n",
    "                vmin=-1,\n",
    "                vmax=1)\n",
    "\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
